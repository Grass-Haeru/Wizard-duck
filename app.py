# Generated by ChatGPT
import sys
import nltk
import wikipedia
from nltk import word_tokenize, pos_tag, ne_chunk, sent_tokenize
from nltk.tree import Tree
from termcolor import colored
from collections import Counter, defaultdict
import os
from flask import Flask, request, jsonify, render_template

app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html')

def kwic_nltk(text, search_value, operation, window, color, sort_mode, next_n=1):
    tokens = word_tokenize(text)
    tagged = pos_tag(tokens)
    entities = ne_chunk(tagged)
    search_tokens = word_tokenize(search_value.lower())
    results = []

    next_word_counter = Counter()
    next_word_examples = defaultdict(list)
    next_word_pos_counter = Counter()
    next_word_pos_examples = defaultdict(list)

    # entity検索用にエンティティの単語位置を取得しておく
    entity_word_positions = set()
    if operation == 3:
        for subtree in entities:
            if isinstance(subtree, Tree) and subtree.label() == search_value:
                for leaf in subtree.leaves():
                    # tokens中のleaf[0]と一致する単語のインデックスを取得
                    # トークン化のズレに注意しつつ単純な検索を行う
                    for i, token in enumerate(tokens):
                        if token == leaf[0]:
                            entity_word_positions.add(i)

    for i in range(len(tokens) - len(search_tokens) + 1):
        match = False

        if operation == 1:
            if [t.lower() for t in tokens[i:i + len(search_tokens)]] == search_tokens:
                match = True
        elif operation == 2:
            if tagged[i][1] == search_value:
                match = True
        elif operation == 3:
            if i in entity_word_positions:
                match = True

        if match:
            start = max(i - window, 0)
            end = min(i + len(search_tokens) + window, len(tokens))
            context = (
                " ".join(tokens[start:i]) +
                ' <span style="color:{};font-weight:bold">'.format(color) +
                " ".join(tokens[i:i + len(search_tokens)]) +
                '</span> ' +
                " ".join(tokens[i + len(search_tokens):end])
            )
            results.append({
                'index': i,
                'context': "... " + context + " ...",
                'token': " ".join(tokens[i:i + len(search_tokens)]),
                'pos': " ".join([tagged[j][1] for j in range(i, i + len(search_tokens))])
            })

            # 次の単語やPOSの収集
            if i + len(search_tokens) < len(tokens):
                next_words = tuple(tokens[i + len(search_tokens):i + len(search_tokens) + next_n])
                next_pos = tuple(tagged[j][1] for j in range(i + len(search_tokens), min(i + len(search_tokens) + next_n, len(tokens))))
                if next_words:
                    next_word_counter[next_words] += 1
                    next_word_examples[next_words].append("... " + " ".join(context) + " ...")
                if next_pos:
                    next_word_pos_counter[next_pos] += 1
                    next_word_pos_examples[next_pos].append("... " + " ".join(context) + " ...")

    if sort_mode == 2:
        # 次語頻度順にresultsを並べる
        freq_order = [k for k, v in next_word_counter.most_common()]
        def get_next_words(res):
            # res['index']を使って次語をtokensから取得
            idx = res['index'] + len(res['token'].split())
            return tuple(tokens[idx:idx+1]) if idx < len(tokens) else ()
        results.sort(key=lambda r: freq_order.index(get_next_words(r)) if get_next_words(r) in freq_order else len(freq_order))
    elif sort_mode == 3:
        freq_order = [k for k, v in next_word_pos_counter.most_common()]
        def get_next_pos(res):
            idx = res['index'] + len(res['token'].split())
            return tuple(tagged[j][1] for j in range(idx, idx+1) if j < len(tagged))
        results.sort(key=lambda r: freq_order.index(get_next_pos(r)) if get_next_pos(r) in freq_order else len(freq_order))
    else:
        results.sort(key=lambda x: x['index'])

    # ソートモードの処理はあなたの既存コードのままでOKです。

    if operation == 1:
        if sort_mode == 2:
            print(f"\nOrder of appearance frequency（「{search_value} + next word」）:")
            for next_words, count in next_word_counter.most_common():
                pair = search_value + " " + " ".join(next_words)
                print(f"{pair}: {count}")
                for example in next_word_examples[next_words]:
                    print(example)
            return {
                'results': results,
                'next_word_counter': next_word_counter,
                'next_word_pos_counter': next_word_pos_counter,
            }
        elif sort_mode == 3:
            print(f"\nOrder of appearance frequency（「{search_value} + <POS of next word>」）:")
            for pos, count in next_word_pos_counter.most_common():
                pos_str = " ".join(pos)
                print(f"{search_value} <{pos_str}>: {count}")
                for example in next_word_pos_examples[pos]:
                    print(example)
            return {
                'results': results,
                'next_word_counter': next_word_counter,
                'next_word_pos_counter': next_word_pos_counter,
            }

    if sort_mode in [2, 3] and operation in [2, 3]:
        if operation == 2 and sort_mode == 2:
            print(f"\nOrder of appearance frequency（<{search_value}> + next word）:")
            for next_words, count in next_word_counter.most_common():
                pair = f"<{search_value}> " + " ".join(next_words)
                print(f"{pair}: {count}")
                for example in next_word_examples[next_words]:
                    print(example)
            return {
                'results': results,
                'next_word_counter': next_word_counter,
                'next_word_pos_counter': next_word_pos_counter,
            }
        if operation == 2 and sort_mode == 3:
            print(f"\nOrder of appearance frequency（<{search_value}> + <POS of next word>）:")
            for pos, count in next_word_pos_counter.most_common():
                pos_str = " ".join(pos)
                print(f"<{search_value}> <{pos_str}>: {count}")
                for example in next_word_pos_examples[pos]:
                    print(example)
            return {
                'results': results,
                'next_word_counter': next_word_counter,
                'next_word_pos_counter': next_word_pos_counter,
            }
        if operation == 3 and sort_mode == 2:
            print(f"\nOrder of appearance frequency（<{search_value}> + next word）:")
            for next_words, count in next_word_counter.most_common():
                pair = f"<{search_value}> " + " ".join(next_words)
                print(f"{pair}: {count}")
                for example in next_word_examples[next_words]:
                    print(example)
            return {
                'results': results,
                'next_word_counter': next_word_counter,
                'next_word_pos_counter': next_word_pos_counter,
            }
        if operation == 3 and sort_mode == 3:
            print(f"\nOrder of appearance frequency（<{search_value}> + <POS of next word>）:")
            for pos, count in next_word_pos_counter.most_common():
                pos_str = " ".join(pos)
                print(f"<{search_value}> <{pos_str}>: {count}")
                for example in next_word_pos_examples[pos]:
                    print(example)
            return {
                'results': results,
                'next_word_counter': next_word_counter,
                'next_word_pos_counter': next_word_pos_counter,
            }

    # 順次表示
    for r in results:
        print(r['context'])

    return {
        'results': results,
        'next_word_counter': next_word_counter,
        'next_word_pos_counter': next_word_pos_counter,
    }    


@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.form

        interest = data.get('Interest', '')
        try:
            text = wikipedia.summary(interest)
        except wikipedia.exceptions.PageError:
            return jsonify({'status': 'error', 'message': 'There is no interest.'})
        except wikipedia.exceptions.DisambiguationError as e:
            return jsonify({'status': 'error', 'message': 'Ambiguous interest', 'candidates': e.options})

        operation = int(data.get('Operation'))
        if operation == 2:
            target = data.get('POS','')
        else:
            target = data.get('Target', '')
        sort_mode = int(data.get('SortMode'))
        window = int(data.get('Window'))

        color_map = {
            1: 'red',
            2: 'blue',
            3: 'green'
        }

        if operation in color_map:
            if operation == 3:
                target = target.upper()
            elif operation == 1:
                target = target.lower()

            result = kwic_nltk(text, target, operation, window, color_map[operation], sort_mode)

            # カウンタや辞書系はシリアライズ可能な形式にする
            result['next_word_counter'] = { " ".join(map(str, k)) if isinstance(k, tuple) else str(k): v for k, v in result['next_word_counter'].items() }
            result['next_word_pos_counter'] = { " ".join(map(str, k)) if isinstance(k, tuple) else str(k): v for k, v in result['next_word_pos_counter'].items() }

            return jsonify({
                'status': 'success',
                'data': {
                    'results': result['results'],  # 詳細情報
                    'next_word_counter': result['next_word_counter'],
                    'next_word_pos_counter': result['next_word_pos_counter']
                }
            })

        else:
            return jsonify({'status': 'error', 'message': 'Unknown operation'})

    except Exception as e:
        print("---- ERROR ----")
        print(e)  # ← ここでエラー内容をコンソールに出力
        print("---- ERROR ----")
        return jsonify({'status': 'error', 'message': f'サーバーエラー: {str(e)}'}), 500
    
if __name__=='__main__':
    if os.environ.get('RENDER')!='true':
        app.run(debug=True, use_reloader=False)